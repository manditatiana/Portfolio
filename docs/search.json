[
  {
    "objectID": "Datasci.html",
    "href": "Datasci.html",
    "title": "Using R and ChatGPT for Qualitative Coding of SAK Laws",
    "section": "",
    "text": "Over the past two decades, the United States has uncovered a disturbing backlog of untested sexual assault kits (SAKs)—forensic evidence collected from survivors of sexual violence. This problem gained national attention in 2009 with the discovery of 11,000 untested kits in a Detroit warehouse, spurring investigations across the country. Root causes included underfunding, a lack of procedural clarity, and systemic disbelief in survivors, particularly from marginalized communities.\nIn response, federal and state policymakers passed several reforms:\n- The Debbie Smith Act (2004) funded DNA evidence testing.\n- The SAFER Act (2013) directed 75% of these funds toward SAKs.\n- The Sexual Assault Kit Initiative (SAKI, 2015) supported jurisdictions in addressing their backlogs.\nMany states followed by enacting laws that mandated testing timelines and survivor notifications.\nWhile this progress is promising, not all states have comprehensive laws. Critically, the way these laws are written may impact how they are implemented and whether they achieve meaningful reform.",
    "crumbs": [
      "Projects",
      "SAK Law Reform Text Analysis"
    ]
  },
  {
    "objectID": "Datasci.html#content-analysis-in-r-using-chatgpt-and-tidytext",
    "href": "Datasci.html#content-analysis-in-r-using-chatgpt-and-tidytext",
    "title": "Using R and ChatGPT for Qualitative Coding of SAK Laws",
    "section": "Content Analysis in R Using ChatGPT and tidytext",
    "text": "Content Analysis in R Using ChatGPT and tidytext\nTo explore whether the language used in state sexual assault kit (SAK) submission laws affects reform outcomes, I conducted a qualitative content analysis using R and ChatGPT. I followed the 7-step content analysis framework by Bernard, Wutich, and Ryan (2017), which guided the full workflow from formulating a research question to applying codes, analyzing results, and interpreting their meaning.\n\nStep 1: Formulate a Research Question\nDo the linguistic themes found in SAK laws predict how many of the 6 End the Backlog pillars a state has achieved?\n\n\nStep 2: Select a Set of Texts\nI compiled a dataset (law_df) that included:\n\nThe State (State)\nThe number of End the Backlog pillars achieved by each state (Pillars_Achieved)\nThe year the law was passed (Law_Year)\nThe name of the law (Law_Name)\nThe text of the law (Law_Text)\n\n\n\nStep 3: Create a Set of Codes (Themes)\nBased on post-structural feminist theory and the Theory of Social Intervention, I created three thematic codes:\n\nSurvivor vs. Victim Language: This theme examines how the law constructs individuals who have experienced sexual violence. While “victim” aligns with legal tradition and emphasizes harm, “survivor” foregrounds agency and resilience. Post-structural feminist theory posits that this linguistic distinction shapes institutional and societal responses by framing whose voice is centered.\nTrauma-Informed Language: This theme assesses whether a statute acknowledges the emotional and psychological impact of sexual violence. It includes terms such as “trauma-informed,” “victim-centered,” and references to sensitive procedures. Laws that use this language aim to minimize retraumatization and reflect care-based interventions aligned with social intervention theory.\nPermissive vs. Directive Language: This theme evaluates whether laws mandate action or offer discretion. For example, “shall submit” reflects directive framing, signaling accountability and uniformity, whereas “may submit” permits variability. Directive language aligns with state efforts to enforce procedural integrity, a key component of successful policy implementation.\n\nWith ChatGPT’s assistance, I created a custom sentiment lexicon used for tidytext analysis. The lexicon was iteratively expanded and cleaned based on law-specific language and advocacy literature.\n\n\nStep 4: Pretest the Variables\nUsing tidytext and ChatGPT, I coded the first 10 states, compared outputs, and refined the lexicon. I removed problematic terms like “victim” and “shall,” and simplified trauma-informed to a binary presence/absence indicator.\n\n\nStep 5: Apply the Codes to All Texts\nTo begin the analysis, I loaded the necessary R packages and set the working directory to where my datasets are stored.\n\nlibrary(tidyverse)      # Tools for data wrangling and visualization\nlibrary(tidytext)       # Tools for working with text data\nlibrary(readxl)         # For reading Excel files\nlibrary(textclean)      # For cleaning messy text\nlibrary(textshape)      # For reshaping and organizing text\nlibrary(ggplot2)        # For making graphs\nlibrary(writexl)        # For writing Excel files\nlibrary(broom)          # For tidying model output\n\nNext, I imported the laws and sentiment lexicon Excel files and cleaned the law text. This included removing special characters, converting text to lowercase, and tokenizing it into individual words.\n\n# 5.1. Load the two datasets: laws and the custom sentiment lexicon\nlaws &lt;- read_excel(\"SAK Sub Laws.xlsx\")\nlexicon &lt;- read_excel(\"custom_sentiment_lexicon_2.xlsx\")\n\n# 5.2. Clean and standardize the text of each law\nlaws_clean &lt;- laws %&gt;%\n  mutate(\n    Law_Text = replace_non_ascii(Law_Text),           # Remove strange/non-standard characters\n    Law_Text = tolower(Law_Text),                     # Make all text lowercase\n    Law_Text = str_replace_all(Law_Text, \"[^a-z\\\\s]\", \" \"),  # Remove punctuation and numbers\n    Law_Text = str_squish(Law_Text)                   # Remove extra spaces between words\n  )\n\n# 5.3. Break each law into individual words (tokens)\nlaws_tokenized &lt;- laws_clean %&gt;%\n  unnest_tokens(output = term, input = Law_Text, token = \"words\")\n\nI then matched the tokenized words from each law with terms in the sentiment lexicon to assign each word a theme-based sentiment label.\n\n# 5.4. Match each word from the laws to a sentiment label in the lexicon\ncoded_laws &lt;- laws_tokenized %&gt;%\n  inner_join(lexicon, by = c(\"term\" = \"word\")) %&gt;%    # Only keep words that are in the lexicon\n  select(State, Pillars_Achieved, Law_Name, Law_Year, term, sentiment)\n\n# 5.5. Count how many times each sentiment appears across all laws\nsentiment_counts &lt;- coded_laws %&gt;%\n  count(sentiment, sort = TRUE)\n\nThis plot shows the frequency of each sentiment category found across all state laws, giving a sense of which themes are most prevalent.\n\n# 5.6. Visualize the frequency of each sentiment category in a bar chart\nggplot(sentiment_counts, aes(x = reorder(sentiment, n), y = n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Frequency of Sentiment Categories in SAK Laws\",\n       x = \"Sentiment Category\",\n       y = \"Count of Terms\")\n\n\n\n\n\n\n\n\nI then created binary indicators to represent whether each law used survivor, trauma-informed, or directive language. These were coded as 1 if any words from the corresponding sentiment category were present in the law and 0 if not. This presence/absence approach was used to avoid over-weighting laws with longer text or redundant phrasing and to simplify interpretation in the regression models. For example, a single mention of trauma-informed principles was sufficient to classify a law as trauma-informed. These variables were then merged with the outcome data on pillar achievement.\n\n\n\nVariable\nType\nMeaning\n\n\n\n\nSurvivor_vs_Victim\nbinary\n1 = survivor language present\n\n\nTraumaInformed\nbinary\n1 = trauma-informed language\n\n\nDirective_vs_Permissive\nbinary\n1 = directive language\n\n\n\n\n# 5.7. Count the number of terms in each sentiment category per state and create binary (1/0) indicators for each theme per state\nstate_summary &lt;- coded_laws %&gt;%\n  count(State, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(\n    Survivor_vs_Victim = if_else(Survivor &gt; 0, 1, 0),\n    TraumaInformed = if_else(`Trauma-Informed` &gt; 0, 1, 0),\n    Directive_vs_Permissive = case_when(\n      Directive &gt; 0 ~ 1,\n      Permissive &gt; 0 ~ 0,\n      TRUE ~ NA_real_\n    )\n  )\n\n# 5.8. Merge theme data with outcomes (Pillars Achieved) for analysis\nmodel_data &lt;- laws_clean %&gt;%\n  select(State, Pillars_Achieved) %&gt;%\n  left_join(state_summary, by = \"State\")\n\nThis step flags any state that was missing theme coding, which was used as a control variable in the regression models.\n\n# Create a variable indicating if theme data is missing for a state\nmodel_data_prepped &lt;- model_data %&gt;%\n  mutate(\n    MissingThemes = if_else(\n      is.na(Survivor_vs_Victim) | is.na(TraumaInformed) | is.na(Directive_vs_Permissive),\n      1, 0\n    )\n  )\n\n\n\nStep 6: Create a Case-by-Case Matrix and Step 7: Analyze It\nI ran two regression models—one to test the impact of theme presence on reform outcomes, and another to assess whether having a law at all mattered.\n\n# Test the effect of each theme on Pillars Achieved\ntheme_model &lt;- lm(\n  Pillars_Achieved ~ Survivor_vs_Victim + TraumaInformed + Directive_vs_Permissive,\n  data = model_data_prepped\n)\nsummary(theme_model)\n\n\nCall:\nlm(formula = Pillars_Achieved ~ Survivor_vs_Victim + TraumaInformed + \n    Directive_vs_Permissive, data = model_data_prepped)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2021 -0.6250  0.3750  0.6037  1.3457 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               4.6543     0.4835   9.626 4.17e-11 ***\nSurvivor_vs_Victim       -0.4229     0.3154  -1.341    0.189    \nTraumaInformed            0.2287     0.3100   0.738    0.466    \nDirective_vs_Permissive   0.7420     0.4962   1.495    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9062 on 33 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.146, Adjusted R-squared:  0.06832 \nF-statistic:  1.88 on 3 and 33 DF,  p-value: 0.1522\n\n# Test whether missing theme data is related to Pillars Achieved\nmissingness_only_model &lt;- lm(Pillars_Achieved ~ MissingThemes, data = model_data_prepped)\nsummary(missingness_only_model)\n\n\nCall:\nlm(formula = Pillars_Achieved ~ MissingThemes, data = model_data_prepped)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4615 -0.4615  0.1206  0.7027  3.5385 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.2973     0.1853  28.592  &lt; 2e-16 ***\nMissingThemes  -2.8358     0.3633  -7.805 4.35e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.127 on 48 degrees of freedom\nMultiple R-squared:  0.5593,    Adjusted R-squared:  0.5501 \nF-statistic: 60.91 on 1 and 48 DF,  p-value: 4.351e-10\n\n\nThe chart below visualizes the estimated effect of each language theme on the number of pillars a state has achieved. Error bars represent 95% confidence intervals.\n\n# Plot estimated effect of each theme on Pillars Achieved\ntidy(theme_model, conf.int = TRUE) %&gt;%                      # Get coefficients with confidence intervals\n  filter(term != \"(Intercept)\") %&gt;%                         # Exclude intercept\n  ggplot(aes(x = reorder(term, estimate), y = estimate)) +  # Plot coefficients\n  geom_col(fill = \"steelblue\") +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  coord_flip() +\n  labs(\n    title = \"Effect of Legal Language Themes and Missingness on Pillars Achieved\",\n    x = \"Predictor\",\n    y = \"Estimated Effect (Linear Coefficient)\"\n  )",
    "crumbs": [
      "Projects",
      "SAK Law Reform Text Analysis"
    ]
  },
  {
    "objectID": "Datasci.html#results-summary",
    "href": "Datasci.html#results-summary",
    "title": "Using R and ChatGPT for Qualitative Coding of SAK Laws",
    "section": "Results Summary",
    "text": "Results Summary\n\nLanguage Themes Alone Did Not Predict Reform\nIn the first regression model, I tested whether the presence of survivor-oriented, trauma-informed, or directive language in a state’s SAK law predicted how many of the six End the Backlog pillars the state had achieved. None of the individual themes reached statistical significance at the p &lt; .05 level. However, the direction and magnitude of the coefficients suggest meaningful patterns worth further exploration:\n\nDirective Language was associated with a positive increase of approximately 0.74 additional pillars. This suggests that laws with more directive mandates may support stronger implementation, potentially by reducing local discretion and encouraging accountability.\nTrauma-Informed Language had a small positive association (+0.23), which aligns with the idea that laws acknowledging survivors’ psychological needs may contribute to more comprehensive reform. However, the estimate was not statistically significant.\nSurvivor-Oriented Language was negatively associated with pillar achievement (–0.42), a counterintuitive finding. One explanation could be that such language is more common in symbolic laws passed by states with limited resources or infrastructure for implementation.\n\n\n\nThe Impact of Not Having a Law at All\nIn the second model, I examined whether simply having a SAK submission law (regardless of its language) made a difference in pillar adoption. The results were much more robust:\n\nStates with no SAK submission law as of January 2025 had 2.84 fewer pillars achieved on average compared to states with a law.\nThis finding was highly statistically significant (p &lt; .001) and accounted for over 55% of the variance in state-level reform progress.\n\nThis result underscores the critical role of legislative action itself, even beyond the specifics of language used.\n\n\nKey Insights\n\nPassing a law is a necessary step for advancing comprehensive reform.\nThe specific wording of laws may still play a role, particularly in the use of directive and trauma-informed language, but more nuanced research is needed.\nLegal language likely interacts with enforcement mechanisms, funding, advocacy, and training efforts—suggesting that language is just one part of a larger ecosystem required to prevent backlogs and support survivors effectively.",
    "crumbs": [
      "Projects",
      "SAK Law Reform Text Analysis"
    ]
  },
  {
    "objectID": "Datasci.html#additional-visualizations-by-theme",
    "href": "Datasci.html#additional-visualizations-by-theme",
    "title": "Using R and ChatGPT for Qualitative Coding of SAK Laws",
    "section": "Additional Visualizations by Theme",
    "text": "Additional Visualizations by Theme\nBelow are supplementary bar charts that display the most frequently occurring terms within each of the three major themes (Survivor vs. Victim, Trauma-Informed, and Permissive vs. Directive). These plots offer a more granular look at how often specific language is used across state laws and provide further context for interpreting the patterns identified in the sentiment analysis.\n\nThe specific wording of laws may still play a role, particularly in the use of directive and trauma-informed language, but more nuanced research is needed.\nLegal language likely interacts with enforcement mechanisms, funding, advocacy, and training efforts—suggesting that language is just one part of a larger ecosystem required to prevent backlogs and support survivors effectively.\n\n\nSurvivor vs Victim Language\n\n# Filter to only keep words labeled as Survivor or Victim language\nsurvivor_victim_terms &lt;- coded_laws %&gt;%\n  filter(sentiment %in% c(\"Survivor\", \"Victim\")) %&gt;%\n  count(sentiment, term, sort = TRUE)\n\n# View a table of these terms and their counts\nprint(survivor_victim_terms)\n\n# A tibble: 11 × 3\n   sentiment term             n\n   &lt;chr&gt;     &lt;chr&gt;        &lt;int&gt;\n 1 Survivor  survivor        38\n 2 Survivor  survivors       10\n 3 Victim    violation        9\n 4 Victim    abuse            8\n 5 Victim    exploitation     3\n 6 Survivor  healing          2\n 7 Victim    complainant      2\n 8 Victim    injury           2\n 9 Survivor  empowered        1\n10 Survivor  growth           1\n11 Victim    harassment       1\n\n# Create a bar chart comparing the frequency of Survivor vs. Victim terms\nggplot(survivor_victim_terms, aes(x = reorder(term, n), y = n, fill = sentiment)) +\n  geom_col(show.legend = TRUE, position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Survivor vs. Victim Language in SAK Laws\",\n    x = \"Term\",\n    y = \"Frequency\",\n    fill = \"Sentiment Category\"\n  )\n\n\n\n\n\n\n\n\n\n\nTrauma-Informed Language\n\n# Keep only terms labeled as Trauma-Informed\ntrauma_terms &lt;- coded_laws %&gt;%\n  filter(sentiment %in% c(\"Trauma-Informed\")) %&gt;%\n  count(sentiment, term, sort = TRUE)\n\n# View the terms and their counts\nprint(trauma_terms)\n\n# A tibble: 4 × 3\n  sentiment       term              n\n  &lt;chr&gt;           &lt;chr&gt;         &lt;int&gt;\n1 Trauma-Informed safety           55\n2 Trauma-Informed control           8\n3 Trauma-Informed privacy           2\n4 Trauma-Informed compassionate     1\n\n# Bar chart of Trauma-Informed terms\nggplot(trauma_terms, aes(x = reorder(term, n), y = n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(\n    title = \"Trauma-Informed Language in SAK Laws\",\n    x = \"Term\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n\nPermissive vs Directive Language\n\n# Filter to include only Permissive and Directive language\nperm_dir_terms &lt;- coded_laws %&gt;%\n  filter(sentiment %in% c(\"Permissive\", \"Directive\")) %&gt;%\n  count(sentiment, term, sort = TRUE)\n\n# View table of terms\nprint(perm_dir_terms)\n\n# A tibble: 20 × 3\n   sentiment  term            n\n   &lt;chr&gt;      &lt;chr&gt;       &lt;int&gt;\n 1 Permissive may           144\n 2 Directive  must           77\n 3 Permissive request        65\n 4 Directive  required       59\n 5 Permissive appropriate    58\n 6 Directive  ensure         29\n 7 Directive  requirement    20\n 8 Permissive allow          14\n 9 Directive  requires       12\n10 Permissive should         12\n11 Directive  will           10\n12 Permissive can            10\n13 Directive  mandatory       8\n14 Permissive could           7\n15 Directive  duty            5\n16 Permissive entitled        3\n17 Permissive permit          3\n18 Permissive authorize       2\n19 Permissive encourage       2\n20 Directive  enforce         1\n\n# Bar chart of Permissive vs. Directive language terms\nggplot(perm_dir_terms, aes(x = reorder(term, n), y = n, fill = sentiment)) +\n  geom_col(show.legend = TRUE, position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Permissive vs. Directive Language in SAK Laws\",\n    x = \"Term\",\n    y = \"Frequency\",\n    fill = \"Sentiment Category\"\n  )",
    "crumbs": [
      "Projects",
      "SAK Law Reform Text Analysis"
    ]
  },
  {
    "objectID": "Datasci.html#references",
    "href": "Datasci.html#references",
    "title": "Using R and ChatGPT for Qualitative Coding of SAK Laws",
    "section": "References",
    "text": "References\n\nBernard, H. R., Wutich, A., & Ryan, G. W. (2017). Analyzing Qualitative Data: Systematic Approaches (2nd ed.). SAGE Publications.\nSilge, J., & Robinson, D. (2017). Text Mining with R: A Tidy Approach. O’Reilly Media.\nEnd the Backlog. (n.d.). The Six Pillars of Reform. Joyful Heart Foundation. Retrieved from https://www.endthebacklog.org/, H. R., Wutich, A., & Ryan, G. W. (2017). Analyzing Qualitative Data: Systematic Approaches (2nd ed.). SAGE Publications.",
    "crumbs": [
      "Projects",
      "SAK Law Reform Text Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mandi Urbizo-Haukjaer",
    "section": "",
    "text": "Mandi Urbizo-Haukjaer is a doctoral candidate in Community and Prevention Research at the University of Illinois Chicago, with a minor in Quantitative Statistics. She holds an M.A. in Applied Psychology from the University of Cincinnati and graduate certificates in Women, Gender, and Sexuality Studies, and Applied Behavioral Analysis. Mandi’s research focuses on improving systems that respond to gender-based violence. She has experience conducting research on topics such as sexual assault response systems and the impact of neoliberal ideologies on public perceptions of gender violence.\n\n\n\n\nIn addition to her research, Mandi has consulted with organizations like the Illinois Coalition Against Sexual Assault (ICASA) to evaluate programs and deliver data-driven insights. She has also designed and facilitated workshops and has presented her work at several conferences. Mandi excels at translating complex statistical methods, such as structural equation modeling and mixed-effects modeling, into accessible language for diverse audiences. Her work aims to promote equity, advocate for marginalized communities, and foster meaningful systemic change."
  },
  {
    "objectID": "EFAworkplace.html",
    "href": "EFAworkplace.html",
    "title": "Intersectional Work Discrimination Scale Development",
    "section": "",
    "text": "Community Psychology is a somewhat young field of psychology, with the coining of the term in 1966 marking the beginning of the discipline (Bennett et al., 1966). Since then, community psychologists (CPs) have integrated and worked in a variety of settings including both academic/research orientated positions and more practice/applied settings. However, unlike other fields, understanding of the experiences of CPs has yet to be fully explored. Thus, a study was conducted by Shaw, Chávez, and Voight to gather information on the types of settings and experiences of community psychologist, specifically of those who are the lone CP in their work unit (see Shaw et al., 2022 for more).\nPart one of the study included a survey, which needed to include a section to measure discrimination to assess this aspect of CPs experiences. However, there are few studies that look at a person’s intersectional identity as whole regarding their experiences with discrimination in the workplace (Scheim & Bauer, 2019). There are some scales that look at specific cross-section of an identity, such as Gendered Racial Microaggressions Scale for Black women and many that looks at racial discrimination (Lewis & Neville, 2015; Williams, 2016). Consequently, as Scheim & Bauer (2019) noted, it has become common to adapt racial discrimination scales to fit a more intercategorical scope, such as with this survey. Yet, by adapting these scales, and in our case also adapting them to also be specific to a work unit, it can limit their validity. Thus, by combining and adapting two scales, one that is intersectional in nature but does not focus on workplace discrimination, and a second one that is racially focused but designed to measure workplace discrimination, we hope to create a scale that captures overall discrimination experiences based on all aspects of a person’s identity in the workplace.",
    "crumbs": [
      "Projects",
      "Intersectional Work Discrimination Scale"
    ]
  },
  {
    "objectID": "EFAworkplace.html#methods",
    "href": "EFAworkplace.html#methods",
    "title": "Intersectional Work Discrimination Scale Development",
    "section": "Methods",
    "text": "Methods\n\nSample\nParticipants were recruited using the SCRA listserv. To qualify for participation, survey takers were required to be 18 years or older and self-identify as a community psychologist. The survey was first sent out on June 14, 2020, on the listserv, with reminders every 2 weeks, until the survey closed on August 31, 2020. A total of 194 responses were collected for this study, but 75 responses were not used in this analysis. Specifically, 55 were taken out due not taking the survey and 20 were taken out due to incomplete surveys. The final sample consisted of 119 participants.\n\n\nMeasures\n\nIntersectional discrimination index.\nThe Intersectional Discrimination Index, specifically the lifetime day-to-day discrimination subscale, (InDI-D; Scheim & Bauer, 2019) was used to measure overall day to day discrimination but was edited to be specific for the work unit. It consists of 9 items and uses a five-point Likert-type scale ranging from 1 (Once a week or more) to 5 (Never). Example items include, “Because of who you are, have you… Heard, saw, or read others in your work unit joking or laughing about you (or people like you),” and “…Been treated as if others in your work unit are afraid of you.”\n\n\nChronic Work Discrimination and Harassment.\nA subset of The Chronic Work Discrimination and Harassment scale (Bennett, Anderson, Cooper, Hassol, & Klein, 1966; Bobo, & Suh, 2000). was used to measure discrimination with items more geared to the work environment. The total scale has 12 items, but only 8 items were used to shorten the length of the survey. Additionally, this scale was made specifically for racial discrimination and was edited to ask about overall discriminatory experience, rather than just those racially targeted. All items use a five-point Likert-type scale ranging from 1 (Once a week or more) to 5 (Never). Example items include, “Because of who you are, have you…Witnessed your supervisor or boss use bigoted slurs or jokes,” and “…Had to work twice as hard as others in your work unit.”\n\n\nDemographics.\nThe participants were asked to identify their race and/or ethnicity based on the planned racial/ethnic categories for the 2020 U.S. Census (allowed to check more than one category), then to identify their race and/or ethnicity in a free response, what country they worked in, if they identify as LBGTQIA+, then to identify their sexual orientation in a free response, their gender identity in a free response, and finally if they have a disability or consider themselves otherly abled.",
    "crumbs": [
      "Projects",
      "Intersectional Work Discrimination Scale"
    ]
  },
  {
    "objectID": "EFAworkplace.html#analysis",
    "href": "EFAworkplace.html#analysis",
    "title": "Intersectional Work Discrimination Scale Development",
    "section": "Analysis",
    "text": "Analysis\n\nClean and visualize data\nAfter loading the necessary packages and data into r, we cleaned the data by using the tiderverse package. We then visualized the data using histograms, tables, and correlation plots to see what the response distrubtion looked like for each item on both scales.\n\n\nExplore Dimensionality\nThen we started the test of dimensionality by conducting a Scree test that yielded three factors above the scree line (Kaiser 1960). However, a parallel analysis with 1000 iterations only found two factors passing the 95% threshold (Horn, 1973). Based on this, an exploratory factor analysis (EFA) with oblemin rotation and two factors was conducted to understand the underlying structure of the 17 items. Most items loaded onto the first factor, with items 103, 104, 105, and 106 from the Chronic Work Discrimination and Harassment scale having the highest loadings on the second factor (0.849, 0.824, 0.498, and 0.793, respectively). These four items all focused on assessing the use of bigoted slurs or jokes use by both supervisors and colleagues.\n\n# Perform exploratory factor analysis (EFA) to identify latent factors underlying the data\n\n# Scree test: Check the eigenvalues to determine the number of factors\ne.values &lt;- eigen(cor(LDF.Discrim.EFA, use=\"pairwise.complete.obs\"))\ne.values$values  # Display eigenvalues\n\n [1] 6.6187640 1.7326647 1.1561030 0.9861073 0.9459382 0.8800107 0.6934930\n [8] 0.6860663 0.5410368 0.5162151 0.4840753 0.4496431 0.3891894 0.2993011\n[15] 0.2777680 0.1984554 0.1451686\n\nplot(1:17, e.values$values, xlab=\"Component Number\", ylab=\"Eigen Values\")  # Plot eigenvalues\nabline(h=1, lty=2)  # Add a horizontal line at 1 to help interpret the scree test\n\n\n\n\n\n\n\n# Parallel analysis: Simulate random data to estimate the number of factors\nParallel.Discrim &lt;- fa.parallel(LDF.Discrim.EFA, fa=\"pc\", n.iter = 1000)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  2 \n\n# EFA with oblimin rotation to allow for correlated factors\nEFA.Discrim &lt;- fa(LDF.Discrim.EFA, nfactors=2)\n\n# Display factor loadings for each item on the factors\nEFA.Discrim$loadings\n\n\nLoadings:\n     MR1    MR2   \nQ91   0.330  0.419\nQ92   0.571       \nQ93   0.120  0.453\nQ94   0.449       \nQ95          0.306\nQ96   0.421  0.306\nQ97   0.332  0.359\nQ98   0.332  0.307\nQ99   0.636 -0.146\nQ100  0.579  0.113\nQ101  0.850       \nQ102  0.670       \nQ103         0.849\nQ104         0.824\nQ105  0.267  0.498\nQ106         0.793\nQ107  0.738       \n\n                 MR1   MR2\nSS loadings    3.584 3.109\nProportion Var 0.211 0.183\nCumulative Var 0.211 0.394\n\n# Display the factor correlation matrix (Phi)\nEFA.Discrim$Phi\n\n          MR1       MR2\nMR1 1.0000000 0.5368325\nMR2 0.5368325 1.0000000\n\n\n\n\nEFA 1\nBased on this, four other exploratory analyses were then conducted that excluded differing items in an effort to identify the most logical set of dimensions. The first variation in analyses only included items from the InDI-D to see if all items loaded onto one factor. Results of the Scree test, parallel analysis, and EFA all demonstrated that this scale loaded onto one factor.\n\n# Data with just InDI-D variables (9 items from the intersectional discrimination scale)\nInDI.Scale &lt;- c(\"Q91\",\"Q92\",\"Q93\",\"Q94\",\"Q95\",\"Q96\",\"Q97\",\"Q98\",\"Q99\")\nLDF.Discrim.InDI &lt;- LDF.Discrim.clean[InDI.Scale]\n\n# Perform Scree test\ne.values2 &lt;- eigen(cor(LDF.Discrim.InDI, use=\"pairwise.complete.obs\"))\nplot(1:9, e.values2$values, xlab=\"Component Number\", ylab=\"Eigen Values\")\nabline(h=1, lty=2)\n\n\n\n\n\n\n\n# Parallel analysis to determine the number of factors\nParallel.Discrim.InDI &lt;- fa.parallel(LDF.Discrim.InDI, fa=\"pc\", n.iter = 1000)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1 \n\n# EFA with oblimin rotation for 1 factor\nEFA.Discrim.InDI &lt;- fa(LDF.Discrim.InDI, nfactors=1)\n\n# Display loadings for the factor analysis\nEFA.Discrim.InDI$loadings\n\n\nLoadings:\n    MR1  \nQ91 0.647\nQ92 0.558\nQ93 0.517\nQ94 0.466\nQ95 0.263\nQ96 0.713\nQ97 0.543\nQ98 0.554\nQ99 0.452\n\n                 MR1\nSS loadings    2.598\nProportion Var 0.289\n\n\n\n\nEFA 2\nSimilarly, the second variation only included items from the Chronic Work Discrimination and Harassment scale. However, the results for this variation showed a two-factor model, with the second factor only loading with items 103, 104, 105, and 106 (see Figure 5, 6, and Table 3). Again, these four questions all focused on the use of bigoted slurs or jokes use by both supervisors and colleagues. Because of this, pairwise correlations were run, which demonstrated that the items were highly correlated with one another.\n\n# Data with just the Chronic Work Discrimination and Harassment Scale (CWDHS variables)\nCWDHS.Scale &lt;- c(\"Q100\",\"Q101\",\"Q102\",\"Q103\",\"Q104\",\"Q105\",\"Q106\",\"Q107\")\nLDF.Discrim.CWDHS &lt;- LDF.Discrim.clean[CWDHS.Scale]\n\n# Perform Scree test for CWDHS scale\ne.values3 &lt;- eigen(cor(LDF.Discrim.CWDHS, use=\"pairwise.complete.obs\"))\nplot(1:8, e.values3$values, xlab=\"Component Number\", ylab=\"Eigen Values\")\nabline(h=1, lty=2)\n\n\n\n\n\n\n\n# Parallel analysis for CWDHS scale\nParallel.Discrim.CWDHS &lt;- fa.parallel(LDF.Discrim.CWDHS, fa=\"pc\", n.iter = 1000)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  2 \n\n# EFA with oblimin rotation for 2 factors\nEFA.Discrim.CWDHS &lt;- fa(LDF.Discrim.CWDHS, nfactors=2)\n\n# Display factor loadings for CWDHS\nEFA.Discrim.CWDHS$loadings\n\n\nLoadings:\n     MR1    MR2   \nQ100  0.665       \nQ101  0.760       \nQ102  0.744       \nQ103  0.181  0.660\nQ104         0.918\nQ105  0.417  0.375\nQ106         0.832\nQ107  0.761       \n\n                 MR1   MR2\nSS loadings    2.368 2.114\nProportion Var 0.296 0.264\nCumulative Var 0.296 0.560\n\n# Analyzing correlations between specific items related to slurs and harassment\nwith(LDF.Discrim.CWDHS, table(Q103,Q104)) # boss use vs at you\n\n    Q104\nQ103   2   3   4   5\n   2   1   0   0   0\n   3   0   1   2   3\n   4   0   0   2   5\n   5   0   1   0 103\n\nwith(LDF.Discrim.CWDHS, table(Q103,Q105)) # boss use vs colleague use\n\n    Q105\nQ103  2  3  4  5\n   2  1  0  0  0\n   3  1  4  1  0\n   4  0  3  3  1\n   5  1  9  6 88\n\nwith(LDF.Discrim.CWDHS, table(Q103,Q106)) # boss use vs colleagues at you\n\n    Q106\nQ103   2   4   5\n   2   1   0   0\n   3   0   2   4\n   4   0   2   5\n   5   0   2 102\n\n# Pairwise correlations of 4 items related to slurs\nslur.items &lt;- c(\"Q103\",\"Q104\",\"Q105\",\"Q106\")\nLDF.Discrim.slurs &lt;- LDF.Discrim.clean[slur.items]\npCor2 &lt;- cor(LDF.Discrim.slurs, use=\"pairwise.complete.obs\")\ncorPlot(pCor2)\n\n\n\n\n\n\n\n\n\n\nEFA 3\nThus, items 103 (witnessing your supervisor or boss use bigoted slurs or jokes) and 106 (having your colleagues direct bigoted slurs or jokes at you) were removed. These two items were excluded as they were highly correlated with both items 104 and 105, thus not adding value and being redundant to the scale (see Figure 7 for correlations). Additionally, leaving 103 and 104 in the scale allows for one item that discusses supervisors and one that discusses colleagues, thus capturing the theory behind the original four questions.\n\n# Data with just both scales excluding Q104 & Q105 based on low correlations\nDiscrim.Scale.2 &lt;- c(\"Q91\",\"Q92\",\"Q93\",\"Q94\",\"Q95\",\"Q96\",\"Q97\",\"Q98\",\"Q99\",\"Q100\",\"Q101\",\"Q102\",\"Q104\",\"Q105\",\"Q107\")\nLDF.Discrim.2 &lt;- LDF.Discrim.clean[Discrim.Scale.2]\n\n# Perform Scree test\ne.values4 &lt;- eigen(cor(LDF.Discrim.2, use=\"pairwise.complete.obs\"))\nplot(1:15, e.values4$values)\nabline(h=1, lty=2)\n\n\n\n\n\n\n\n# Parallel analysis for both scales\nParallel.Discrim.2 &lt;- fa.parallel(LDF.Discrim.2, fa=\"pc\", n.iter = 1000)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1 \n\n# EFA with oblimin rotation for 1 factor\nEFA.Discrim.2 &lt;- fa(LDF.Discrim.2, nfactors=1)\n\n# Display factor loadings\nEFA.Discrim.2$loadings\n\n\nLoadings:\n     MR1  \nQ91  0.639\nQ92  0.546\nQ93  0.464\nQ94  0.485\nQ95  0.277\nQ96  0.644\nQ97  0.578\nQ98  0.559\nQ99  0.485\nQ100 0.641\nQ101 0.775\nQ102 0.654\nQ104 0.509\nQ105 0.630\nQ107 0.710\n\n                 MR1\nSS loadings    5.130\nProportion Var 0.342\n\n\n\n\nEFA 4\nThe final variation also removed item 95 (been stared or pointed at by others in your work unit), as with both the initial full item EFA and with the InDI-D only EFA, it had a factor loading of less than 0.30. Thus, the final EFA ran with 14 items on one factor, which can be conceptualized as intersectional work discrimination, with all loadings above 0.4. This factor explained 36.1% if the total variance.\n\n# Data with both scales excluding Q95, Q104 & Q105 based on factor analysis and redundancy\nDiscrim.Scale.3 &lt;- c(\"Q91\",\"Q92\",\"Q93\",\"Q94\",\"Q96\",\"Q97\",\"Q98\",\"Q99\",\"Q100\",\"Q101\",\"Q102\",\"Q104\",\"Q105\",\"Q107\")\nLDF.Discrim.3 &lt;- LDF.Discrim.clean[Discrim.Scale.3]\n\n# Perform Scree test\ne.values5 &lt;- eigen(cor(LDF.Discrim.3, use=\"pairwise.complete.obs\"))\nplot(1:14, e.values5$values, xlab=\"Component Number\", ylab=\"Eigen Values\")\nabline(h=1, lty=2)\n\n\n\n\n\n\n\n# Parallel analysis for the final scale\nParallel.Discrim.3 &lt;- fa.parallel(LDF.Discrim.3, fa=\"pc\", n.iter = 1000)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1 \n\n# EFA with oblimin rotation for 1 factor\nEFA.Discrim.3 &lt;- fa(LDF.Discrim.3, nfactors=1)\n\n# Display final factor loadings\nEFA.Discrim.3$loadings\n\n\nLoadings:\n     MR1  \nQ91  0.642\nQ92  0.547\nQ93  0.458\nQ94  0.488\nQ96  0.647\nQ97  0.576\nQ98  0.548\nQ99  0.496\nQ100 0.647\nQ101 0.778\nQ102 0.658\nQ104 0.504\nQ105 0.619\nQ107 0.711\n\n                 MR1\nSS loadings    5.054\nProportion Var 0.361",
    "crumbs": [
      "Projects",
      "Intersectional Work Discrimination Scale"
    ]
  },
  {
    "objectID": "EFAworkplace.html#discussion",
    "href": "EFAworkplace.html#discussion",
    "title": "Intersectional Work Discrimination Scale Development",
    "section": "Discussion",
    "text": "Discussion\nAs the community psychology field continues to grow, and the workforce continues to diversify, understanding employees’ experiences with discrimination across intersecting identities will become increasingly important. While this scale developed used a sample of only community psychologist, they do work in a variety of settings and are often the only CP in their setting which can be further isolating, exacerbating any discriminatory experiences. By combining these two scales, we are better able to gather information on people’s experiences with discrimination based on multiple parts of individual’s identity, while also keeping it brief.\nOverall, these results support the day-to-day subscale of the InDI, which had not been analyzed in this manner before. However, this also demonstrated that redundancy of some items in the Chronic Work Discrimination and Harassment scale. Specifically, the questions regarding the use of bigoted slurs and jokes in the workplace. Based on this, reducing these questions two to questions either focusing on supervisor versus colleagues, or general use versus comments directed at the individual may serve to provide more nuanced information without redundancy. Some limitations of this study are again, the sample being solely made of CPs.\nTo further validate this scale, another sample with individuals from various fields would showcase if these results would hold outside of community psychology. Additionally, getting a larger sample to be would allow for both a derivative and validation sample. This would allow for better assessment of the scale’s validity. Moreover, in this survey, follow-up questions were asked after each item to see what identity was being targeted by the discriminatory action. This should be further analyzed to see how if the results of scale are gathering information on an array of discriminatory experiences rather than only one (such as solely measuring discriminatory experiences based on race). This would be a good avenue for further research.",
    "crumbs": [
      "Projects",
      "Intersectional Work Discrimination Scale"
    ]
  },
  {
    "objectID": "EFAworkplace.html#references",
    "href": "EFAworkplace.html#references",
    "title": "Intersectional Work Discrimination Scale Development",
    "section": "References",
    "text": "References\nBennett, C. C., Anderson, L. S., Cooper, S., Hassol, L., & Klein, D. C. (1966). Community Psychology: A Report of the Boston Conference on the Education of Psychologists for Community Mental Health. Boston: Boston Univ. 8: South Shore Mental Health Center Biglan A, Ary D. Koehn V, Levings D, Smith S, et al. 1996. Mobilizing positive reinforcement in communities to reduce youth access to tobacco. Am. J. Community Psychol, 24, 625-38.\nBobo, L. D., & Suh, S. (2000). Surveying Racial Discrimination: Analyses from a Multiethnic Labor Market. Lawrence D. Bobo, Melvin L. Oliver, James H. Johnson, Jr., and Abel Valenzuela, Jr.~ Eds, 523-560.\nGerbing, D.W. (2021). lessR: Less Code, More Results. R package version 4.0.3, https://cran.r-project.org/web/packages/lessR/index.html.\nHorn, W. A. (1973). Minimizing average flow time with parallel machines. Operations Research, 21(3), 846-847.\nLewis, J.A., Neville, H.A., 2015. Construction and initial validation of the Gendered Racial Microaggressions Scale for Black women. J. Counsel. Psychol. 62, 289–302. https://doi.org/10.1037/cou0000062.\nMcNeilly, M.D., Anderson, N.B., Armstead, C.A., Clark, R., Corbett, M., Robinson, E.L., Pieper, C.F. & Lepisto, E.M. (1996). “The perceived racism scale: A multidimensional assessment of the experience of white racism among African Americans.” Ethnicity and Disease. 6, 154-166.\nScheim, A. I., & Bauer, G. R. (2019). The Intersectional Discrimination Index: Development and validation of measures of self-reported enacted and anticipated discrimination for intercategorical analysis. Social Science & Medicine, 226, 225-235.\nShaw, J., Bailey, C., Danylkiv, A., Kashyap, M., Chávez, N. R., & Voight, A. (2023). The work experiences and needs of lone community psychologists: Exploring diversity of settings and identities. Journal of Community Psychology, 51(5), 1917-1934. https://doi.org/10.1002/jcop.22979\nR Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nRevelle, W. (2021) psych: Procedures for Personality and Psychological Research, Northwestern University, Evanston, Illinois, USA, https://cran.r-project.org/web/packages/psych/index.html,.\nWilliams, D.R. (2016). Measuring discrimination resource. Scholars at Harvard. https://scholar.harvard.edu/files/davidrwilliams/files/measuring_discrimination_resource_june_2016.pdf",
    "crumbs": [
      "Projects",
      "Intersectional Work Discrimination Scale"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Project Summaries",
    "section": "",
    "text": "This study aims to develop and validate a comprehensive scale for measuring intersectional work discrimination across multiple identities in community psychologists’ work units. We adapted and combined two existing scales—the Intersectional Discrimination Index (InDI-D) and the Chronic Work Discrimination and Harassment scale (CWDHS)—to assess the day-to-day and chronic discriminatory experiences faced by individuals in their professional environments. Through exploratory factor analysis (EFA) and parallel analysis, we identified key dimensions of discrimination based on gender, race, and other intersecting identities. The final factor analysis resulted in a one-factor model capturing the core of intersectional work discrimination, with all factor loadings above 0.4. Our findings emphasize the importance of considering multiple identity categories when studying workplace discrimination, particularly in professional settings where individuals may experience unique forms of exclusion and bias. This scale offers a robust tool for future research on work unit discrimination and can be adapted for use in various organizational contexts.",
    "crumbs": [
      "Projects",
      "Project Summaries"
    ]
  },
  {
    "objectID": "about.html#intersectional-work-discrimination-scale-development",
    "href": "about.html#intersectional-work-discrimination-scale-development",
    "title": "Project Summaries",
    "section": "",
    "text": "This study aims to develop and validate a comprehensive scale for measuring intersectional work discrimination across multiple identities in community psychologists’ work units. We adapted and combined two existing scales—the Intersectional Discrimination Index (InDI-D) and the Chronic Work Discrimination and Harassment scale (CWDHS)—to assess the day-to-day and chronic discriminatory experiences faced by individuals in their professional environments. Through exploratory factor analysis (EFA) and parallel analysis, we identified key dimensions of discrimination based on gender, race, and other intersecting identities. The final factor analysis resulted in a one-factor model capturing the core of intersectional work discrimination, with all factor loadings above 0.4. Our findings emphasize the importance of considering multiple identity categories when studying workplace discrimination, particularly in professional settings where individuals may experience unique forms of exclusion and bias. This scale offers a robust tool for future research on work unit discrimination and can be adapted for use in various organizational contexts.",
    "crumbs": [
      "Projects",
      "Project Summaries"
    ]
  },
  {
    "objectID": "about.html#sak-law-reform-text-analysis",
    "href": "about.html#sak-law-reform-text-analysis",
    "title": "Project Summaries",
    "section": "SAK Law Reform Text Analysis",
    "text": "SAK Law Reform Text Analysis\nThis project applies a qualitative content analysis to U.S. state-level Sexual Assault Kit (SAK) submission laws to examine whether the language used in legislation predicts the adoption of reform measures. Using Bernard, Wutich, and Ryan’s (2017) 7-step content analysis framework, I combined ChatGPT and R’s tidytext package to code each law according to three theoretically grounded themes: Survivor vs. Victim Language, Trauma-Informed Language, and Permissive vs. Directive Language. A custom sentiment lexicon was developed through iterative prompting and refinement with ChatGPT. Each law was analyzed to identify the presence or absence of thematic language, and regression models were used to test whether these themes predicted the number of pillars a state achieved, as defined by End the Backlog. Results suggest that while language alone did not significantly predict outcomes, directive and trauma-informed language showed positive trends, and the presence of any SAK law was strongly associated with reform progress. This analysis demonstrates the potential of integrating AI-assisted coding with R for critical policy analysis.",
    "crumbs": [
      "Projects",
      "Project Summaries"
    ]
  },
  {
    "objectID": "about.html#religious-hospitals-and-toxicology-and-emergency-conceptions-administration-during-sak",
    "href": "about.html#religious-hospitals-and-toxicology-and-emergency-conceptions-administration-during-sak",
    "title": "Project Summaries",
    "section": "Religious Hospitals and Toxicology and Emergency Conceptions Administration During SAK",
    "text": "Religious Hospitals and Toxicology and Emergency Conceptions Administration During SAK\nWork in Progress, check back later.",
    "crumbs": [
      "Projects",
      "Project Summaries"
    ]
  }
]